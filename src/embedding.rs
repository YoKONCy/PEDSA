use candle_core::{Device, Tensor};
use crate::inference_engine::QuantizedBertModel as QBertModel;
use tokenizers::{Tokenizer, PaddingParams, TruncationParams};
use std::sync::{Arc, Mutex};
use std::path::PathBuf;

/// åŸºäº Candle çš„åµŒå…¥æ¨¡å‹ (æ”¯æŒ BGE-Small-ZH GGUF)
/// 
/// æ”¯æŒçš„æ¨¡å‹:
/// 1. BGE-Small-ZH (GGUF é‡åŒ–, 512 ç»´)
pub struct CandleModel {
    model: Arc<Mutex<QBertModel>>,
    tokenizer: Tokenizer,
    pub dimension: usize,
}

impl CandleModel {
    /// åˆå§‹åŒ–æ¨¡å‹ (ä»…æ”¯æŒ BGE-Small-ZH GGUF)
    pub fn new() -> Result<Self, Box<dyn std::error::Error>> {
        // ä¼˜å…ˆçº§ 1: BGE-Small-ZH GGUF F16 (æœ€ä½³æ€§èƒ½, ~0.2s å»¶è¿Ÿ)
        // æ³¨æ„: å¯¹äºå°æ¨¡å‹ï¼ŒF16 æ¯” Q8_0 æ›´å¿«ï¼Œå› ä¸ºåé‡åŒ–å¼€é”€æ›´å°
        let bge_small_f16 = "models/bge-small-zh-v1.5-gguf/bge-small-zh-v1.5-f16.gguf";
        if PathBuf::from(bge_small_f16).exists() {
             println!("ğŸ” Found local model: {}", bge_small_f16);
             return Self::load_quantized_gguf(bge_small_f16);
        }

        // ä¼˜å…ˆçº§ 2: BGE-Small-ZH GGUF Q8_0 (ä½“ç§¯æ›´å°, æ¨ç†è¾ƒæ…¢ ~1.8s)
        let bge_small_q8 = "models/bge-small-zh-v1.5-gguf/bge-small-zh-v1.5-q8_0.gguf";
        if PathBuf::from(bge_small_q8).exists() {
             println!("ğŸ” Found local model: {}", bge_small_q8);
             return Self::load_quantized_gguf(bge_small_q8);
        }

        Err("âŒ No supported model found. Please download BGE-Small GGUF.".into())
    }

    /// åŠ è½½é‡åŒ– GGUF æ¨¡å‹
    pub fn load_quantized_gguf(model_dir: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let model_path = PathBuf::from(model_dir);

        // æ£€æŸ¥ model_dir æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
        let is_file = model_path.extension().map_or(false, |ext| ext == "gguf");

        // GGUF æ¨¡å‹æ–‡ä»¶
        let weights_filename = if is_file {
            model_path.clone()
        } else {
            model_path.join("bge-m3-q4_k_m.gguf")
        };

        let search_dir = if is_file {
            model_path.parent().unwrap()
        } else {
            &model_path
        };

        // Tokenizer é…ç½®
        let tokenizer_filename = if search_dir.join("tokenizer.json").exists() {
            search_dir.join("tokenizer.json")
        } else if search_dir.parent().unwrap().join("tokenizer.json").exists() {
             search_dir.parent().unwrap().join("tokenizer.json")
        } else {
             // å°è¯•å‘ä¸ŠæŸ¥æ‰¾ä¸€çº§
             let parent = search_dir.parent().unwrap();
             if parent.parent().unwrap().join("tokenizer.json").exists() {
                 parent.parent().unwrap().join("tokenizer.json")
             } else {
                 return Err("âŒ tokenizer.json not found in model directory".into());
             }
        };

        println!("ğŸ—ï¸ Initializing Candle Quantized Model...");
        println!("ğŸ“‚ Loading weights from: {:?}", weights_filename);

        if !weights_filename.exists() {
            return Err(format!("âŒ Weights file not found: {:?}", weights_filename).into());
        }

        let model = QBertModel::new(weights_filename.to_str().unwrap())?;
        let tokenizer = Tokenizer::from_file(tokenizer_filename).map_err(|e| e.to_string())?;

        // ä»æ¨¡å‹è·å–ç»´åº¦
        let hidden_size = model.hidden_size(); 
        
        println!("âœ… Model loaded successfully. Hidden size: {}", hidden_size);

        Ok(Self {
            model: Arc::new(Mutex::new(model)),
            tokenizer,
            dimension: hidden_size,
        })
    }

    /// æ‰§è¡Œå‘é‡åŒ– (æ¨ç†)
    pub fn vectorize_weighted(&self, text: &str, _weighted_ranges: &[(usize, usize, f32)]) -> Option<Vec<f32>> {
        let device = Device::Cpu;
        let mut tokenizer = self.tokenizer.clone();
        
        // é…ç½®å¡«å……
        if let Some(pp) = tokenizer.get_padding_mut() {
            pp.strategy = tokenizers::PaddingStrategy::BatchLongest;
        } else {
            let pp = PaddingParams {
                strategy: tokenizers::PaddingStrategy::BatchLongest,
                ..Default::default()
            };
            tokenizer.with_padding(Some(pp));
        }

        // é…ç½®æˆªæ–­
        if let Some(tp) = tokenizer.get_truncation_mut() {
            tp.max_length = 512;
        } else {
            let tp = TruncationParams {
                max_length: 512,
                ..Default::default()
            };
            let _ = tokenizer.with_truncation(Some(tp));
        }

        // åˆ†è¯
        let tokens = match tokenizer.encode(text, true) {
            Ok(t) => t,
            Err(e) => {
                eprintln!("âŒ Tokenizer error: {}", e);
                return None;
            }
        };
        let token_ids = match Tensor::new(tokens.get_ids(), &device) {
            Ok(t) => match t.unsqueeze(0) {
                Ok(t) => t,
                Err(e) => {
                    eprintln!("âŒ Tensor unsqueeze error: {}", e);
                    return None;
                }
            },
            Err(e) => {
                eprintln!("âŒ Tensor creation error: {}", e);
                return None;
            }
        };
        let token_type_ids = match token_ids.zeros_like() {
            Ok(t) => t,
            Err(e) => {
                eprintln!("âŒ Tensor zeros_like error: {}", e);
                return None;
            }
        };

        // å‰å‘ä¼ æ’­
        let embeddings = {
            let model = self.model.lock().unwrap();
            match model.forward(&token_ids, Some(&token_type_ids)) {
                Ok(e) => e,
                Err(e) => {
                    eprintln!("âŒ Quantized Model forward error: {}", e);
                    return None;
                }
            }
        };

        // æ± åŒ–ç­–ç•¥: CLS Token (ç´¢å¼• 0)
        // embeddings å½¢çŠ¶: [1, seq_len, hidden_size]
        let cls_embedding = match embeddings.get(0) {
            Ok(e) => match e.get(0) {
                Ok(c) => c,
                Err(e) => {
                    eprintln!("âŒ Embedding get(0) error: {}", e);
                    return None;
                }
            },
            Err(e) => {
                eprintln!("âŒ Embeddings get(0) error: {}", e);
                return None;
            }
        };
        
        // å½’ä¸€åŒ– (L2)
        let l2_norm = match cls_embedding.sqr() {
            Ok(s) => match s.sum_all() {
                Ok(sum) => match sum.sqrt() {
                    Ok(sqrt) => sqrt,
                    Err(e) => {
                        eprintln!("âŒ Sqrt error: {}", e);
                        return None;
                    }
                },
                Err(e) => {
                    eprintln!("âŒ Sum all error: {}", e);
                    return None;
                }
            },
            Err(e) => {
                eprintln!("âŒ Sqr error: {}", e);
                return None;
            }
        };
        let normalized = match cls_embedding.broadcast_div(&l2_norm) {
            Ok(n) => n,
            Err(e) => {
                eprintln!("âŒ Normalization error: {}", e);
                return None;
            }
        };

        let vec: Vec<f32> = match normalized.flatten_all() {
            Ok(f) => match f.to_vec1() {
                Ok(v) => v,
                Err(e) => {
                    eprintln!("âŒ To vec1 error: {}", e);
                    return None;
                }
            },
            Err(e) => {
                eprintln!("âŒ Flatten all error: {}", e);
                return None;
            }
        };
        
        Some(vec)
    }

    /// å…¼å®¹æ¥å£
    pub fn vectorize(&self, text: &str) -> Option<Vec<f32>> {
        self.vectorize_weighted(text, &[])
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_local_model_loading() {
        println!("Testing local model loading...");
        // å¦‚æœæµ‹è¯•ç¯å¢ƒä¸­æ²¡æœ‰å®Œæ•´è·¯å¾„ï¼Œåˆ™ä½¿ç”¨ç®€åŒ–è·¯å¾„è¿›è¡Œæµ‹è¯•
        // ä½†æˆ‘ä»¬æœŸæœ›å®ƒèƒ½å·¥ä½œã€‚
        let model = CandleModel::new();
        if let Ok(m) = model {
            println!("Model loaded successfully!");
            let vec = m.vectorize("Hello world");
            assert!(vec.is_some());
            let v = vec.unwrap();
            assert_eq!(v.len(), 512);
            println!("Vector sample: {:?}", &v[0..5]);
        } else {
            eprintln!("Model failed to load. Ensure model files are in 'models/bge-m3-gguf/Embedding-GGUF/bge-m3-Q4_K_M-GGUF'");
        }
    }
}
